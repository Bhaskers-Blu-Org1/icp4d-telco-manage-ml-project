{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Time Series Analysis Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Install the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ibm_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Restart the Kernel `Kernel>Restart`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Import the Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import ibm_db\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.stattools import acf, pacf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Insert the Db Credentials\n",
    "\n",
    "Ensure to rename as `credentials_1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code contains the credentials for a connection in your Project.\n",
    "# You might want to remove those credentials before you share your notebook.\n",
    "\n",
    "from project_lib import Project\n",
    "project = Project.access()\n",
    "credentials_1 = project.get_connection(name=\"Smruthi Db2 on Cloud\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Insert the Dataset\n",
    "\n",
    "Click on the `10/01` icon and then click on `Insert Pandas Dataframe`. Ensure the name is `df_data_1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Prepare dataset for Time-Series Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make column datatype a datetime object\n",
    "df_data_1['Start_Time_MM_DD_YYYY'] = pd.to_datetime(df_data_1.Start_Time_MM_DD_YYYY , format = '%Y%m%d')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the date column as index for dataset\n",
    "data = df_data_1.drop(['Start_Time_MM_DD_YYYY'], axis=1)\n",
    "data.index = df_data_1.Start_Time_MM_DD_YYYY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' NOTE: This example is given so that, ensure there are no nan's in your dataset \n",
    "    and they are replaced with type consistent missing values\n",
    "\n",
    "data[' _dewptm']=data[' _dewptm'].fillna(1000)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Time-Series Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Train your AR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_data=data['Call Dropped']\n",
    "ts_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_log = np.log(ts_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_log_diff = ts_log - ts_log.shift()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_log_diff.dropna(inplace=True)\n",
    "ts_log_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARIMA(ts_log, order=(2, 1, 0))  \n",
    "results_AR = model.fit(disp=-1)  \n",
    "plt.plot(ts_log_diff)\n",
    "plt.plot(results_AR.fittedvalues, color='red')\n",
    "plt.title('RSS: %.4f'% sum((results_AR.fittedvalues-ts_log_diff)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ARIMA_diff = pd.Series(results_AR.fittedvalues, copy=True)\n",
    "print(predictions_ARIMA_diff.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ARIMA_diff_cumsum = predictions_ARIMA_diff.cumsum()\n",
    "predictions_ARIMA_diff_cumsum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ARIMA_log = pd.Series(ts_log.ix[0], index=ts_log.index)\n",
    "predictions_ARIMA_log = predictions_ARIMA_log.add(predictions_ARIMA_diff_cumsum,fill_value=0)\n",
    "predictions_ARIMA_log.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ARIMA = np.exp(predictions_ARIMA_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ARIMA=predictions_ARIMA.groupby('Start_Time_MM_DD_YYYY').head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Prepare the Dataset for Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.groupby('outgoing_site_id').head(6)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data=pd.merge(data,pd.DataFrame({\"Call Drop Predictions\":predictions_ARIMA.astype(int)}), how='inner', left_index=True, right_index=True)\n",
    "merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data['Predicted_Call_Drop_Perc']=round((merged_data['Call Drop Predictions']/merged_data['Total Calls'])*100,2)\n",
    "merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data=merged_data.groupby(['outgoing_site_id','Start_Time_HH_MM_SS_s']).head(1)\n",
    "merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data=merged_data.reset_index()\n",
    "del merged_data['Start_Time_MM_DD_YYYY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = merged_data.replace(np.nan, 0, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Store Data back to the Db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsn_driver = \"IBM DB2 ODBC DRIVER\"\n",
    "dsn_database = credentials_1['database'] \n",
    "dsn_hostname = credentials_1['host']\n",
    "dsn_port = 50000               \n",
    "dsn_uid = credentials_1['username']      \n",
    "dsn_pwd = credentials_1['password']\n",
    "\n",
    "dsn = (\n",
    "    \"DRIVER={{IBM DB2 ODBC DRIVER}};\"\n",
    "    \"DATABASE=\"+str(dsn_database)+\";\"\n",
    "    \"HOSTNAME=\"+str(dsn_hostname)+\";\"\n",
    "    \"PORT=\"+str(dsn_port)+\";\"\n",
    "    \"PROTOCOL=TCPIP;\"\n",
    "    \"UID=\"+str(dsn_uid)+\";\"\n",
    "    \"PWD=\"+str(dsn_pwd)+\";\").format(dsn_database, dsn_hostname, dsn_port, dsn_uid, dsn_pwd)\n",
    "\n",
    "\n",
    "\n",
    "conn = ibm_db.connect(dsn, \"\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: Replace <Schema_Name> with an existing Schema name in your db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the Schema Name with the actual name\n",
    "create_statement=pd.io.sql.get_schema(merged_data.reset_index(), 'TIMESERIES_DATA')\n",
    "create_statement=create_statement.replace('TEXT', 'VARCHAR(500)')\n",
    "create_statement=create_statement.replace('TIMESTAMP', 'VARCHAR(500)')\n",
    "#create_statement= create_statement + \"IN \"+dsn_database\n",
    "print(create_statement)\n",
    "ibm_db.exec_immediate(conn, create_statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple_of_tuples = tuple([tuple(x) for x in merged_data.values])\n",
    "i=1\n",
    "for x in merged_data.values:\n",
    "    vals= (i,) + tuple(x)\n",
    "    print(vals)\n",
    "    sql = \"INSERT INTO TIMESERIES_DATA VALUES\"+ str(vals)\n",
    "    i=i+1\n",
    "    ins_sql=ibm_db.prepare(conn, sql)\n",
    "    ibm_db.execute(ins_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
